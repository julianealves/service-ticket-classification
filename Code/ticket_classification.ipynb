{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook trains a BERT model for IT Service ticket classification \n",
    "##### The BERT model is from HuggingFace and the dataset is from Kaggle (https://www.kaggle.com/datasets/adisongoh/it-service-ticket-classification-dataset?resource=download). The dataset has 8 classes (Access, Administrative rights, HR Support, Hardware, Internal Project, Miscellaneous, Purchase, Storage) and one feature (Document). The document column has the IT ticket description. See EDA notebook for more details about the data.\n",
    "\n",
    "##### **Final results:**\n",
    "|   Metric  |   Value  |\n",
    "|:---------:|:--------:|\n",
    "|     f1    |  0.85765 |\n",
    "|  roc_auc  | 0.918657 |\n",
    "|  accuracy |  0.85765 |\n",
    "| precision |  0.85765 |\n",
    "|   recall  |  0.85765 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "from datasets import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL = \"distilbert/distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/all_tickets_processed_improved_v3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[\"Document\"], data[\"Topic_group\"], test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_binarizer = LabelBinarizer()\n",
    "y_train_encoded = label_binarizer.fit_transform(y_train)\n",
    "y_test_encoded = label_binarizer.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train and test tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(data):\n",
    "    return tokenizer(data[\"text\"], truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = Dataset.from_pandas(pd.DataFrame({\"text\": X_train, \"label\": y_train_encoded.astype(np.float32).tolist()}))\n",
    "test_df = Dataset.from_pandas(pd.DataFrame({\"text\": X_test, \"label\": y_test_encoded.astype(np.float32).tolist()})) \n",
    "\n",
    "tokenized_train_data = train_df.map(preprocess_function, batched=True)\n",
    "tokenized_test_data = test_df.map(preprocess_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(BERT_MODEL, num_labels=y_train.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training and evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(true_label, pred_label):\n",
    "\n",
    "    f1_micro_average = f1_score(true_label, pred_label, average=\"micro\")\n",
    "    roc_auc = roc_auc_score(true_label, pred_label, average = \"micro\")\n",
    "    accuracy = accuracy_score(true_label, pred_label)\n",
    "    precision = precision_score(true_label, pred_label, average=\"micro\")\n",
    "    recall = recall_score(true_label, pred_label, average=\"micro\")\n",
    "\n",
    "    metrics = {\"f1\": f1_micro_average,\n",
    "            \"roc_auc\": roc_auc,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall\n",
    "            }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def from_probabilities_to_binarizer(probs):\n",
    "    return [1 if p == probs[np.argmax(probs)] else 0 for p in probs]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2e-5\n",
    "num_train_epochs = 15\n",
    "weight_decay = 0.01\n",
    "\n",
    "with mlflow.start_run():\n",
    "\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "    mlflow.log_param(\"num_train_epochs\", num_train_epochs)\n",
    "    mlflow.log_param(\"weight_decay\", weight_decay)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "    output_dir=\"./ticket_classification\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "    \n",
    "    trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_data,\n",
    "    eval_dataset=tokenized_test_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "    \n",
    "    trainer.train()\n",
    " \n",
    "    preds = trainer.predict(tokenized_test_data)\n",
    "\n",
    "    y_pred_probs = [list(row) for row in preds.predictions]\n",
    "    y_pred = np.array(list(map(from_probabilities_to_binarizer, y_pred_probs)))\n",
    "\n",
    "    metrics = compute_metrics(y_test_encoded, y_pred)\n",
    "\n",
    "    print(metrics)\n",
    "\n",
    "    mlflow.log_metrics(metrics)\n",
    "\n",
    "    components = {\n",
    "        \"model\": model,\n",
    "        \"tokenizer\": tokenizer,\n",
    "    }\n",
    "\n",
    "    mlflow.transformers.log_model(\n",
    "        transformers_model=components,\n",
    "        artifact_path=\"model\",\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
